{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram  data_merge.txt   data_seg3.txt  model.py\r\n",
      "\u001b[0m\u001b[01;34mcode\u001b[0m/   data_process.py  data_seg.txt   stop_list.txt\r\n",
      "\u001b[01;34mdata\u001b[0m/   data_seg2.txt    filterwords    time_process.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = pd.read_csv('./data/2016_1.csv', usecols=['FD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-01-06    65\n",
       "2016-01-27    60\n",
       "2016-01-20    56\n",
       "2016-01-13    52\n",
       "2016-02-24    13\n",
       "2016-02-17     4\n",
       "Name: FD, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.FD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2016-01-06\n",
       "1     2016-01-13\n",
       "2     2016-01-06\n",
       "3     2016-01-06\n",
       "4     2016-01-06\n",
       "5     2016-01-06\n",
       "6     2016-01-13\n",
       "7     2016-01-06\n",
       "8     2016-01-13\n",
       "9     2016-01-13\n",
       "10    2016-01-13\n",
       "11    2016-01-13\n",
       "12    2016-01-13\n",
       "13    2016-01-13\n",
       "14    2016-01-13\n",
       "15    2016-01-13\n",
       "16    2016-01-13\n",
       "17    2016-01-13\n",
       "18    2016-01-13\n",
       "19    2016-01-13\n",
       "20    2016-01-13\n",
       "21    2016-01-13\n",
       "22    2016-01-13\n",
       "23    2016-01-06\n",
       "24    2016-01-06\n",
       "25    2016-01-06\n",
       "26    2016-01-06\n",
       "27    2016-01-13\n",
       "28    2016-01-13\n",
       "29    2016-01-06\n",
       "         ...    \n",
       "220   2016-01-20\n",
       "221   2016-01-20\n",
       "222   2016-01-20\n",
       "223   2016-01-27\n",
       "224   2016-01-27\n",
       "225   2016-01-27\n",
       "226   2016-01-27\n",
       "227   2016-01-27\n",
       "228   2016-01-27\n",
       "229   2016-01-27\n",
       "230   2016-01-27\n",
       "231   2016-01-27\n",
       "232   2016-01-27\n",
       "233   2016-02-17\n",
       "234   2016-02-24\n",
       "235   2016-02-24\n",
       "236   2016-02-24\n",
       "237   2016-02-24\n",
       "238   2016-02-24\n",
       "239   2016-02-17\n",
       "240   2016-02-24\n",
       "241   2016-02-24\n",
       "242   2016-02-24\n",
       "243   2016-02-24\n",
       "244   2016-02-24\n",
       "245   2016-02-24\n",
       "246   2016-02-24\n",
       "247   2016-02-24\n",
       "248   2016-02-17\n",
       "249   2016-02-17\n",
       "Name: FD, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(date.FD, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdate = pd.to_datetime(date.FD, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdate = list(rdate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str1 = '2016-02-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transDate(date_column):\n",
    "    date = []\n",
    "    for i in list(date_column):\n",
    "        year = int(i[:4])\n",
    "        month = int(i[5:7])\n",
    "        day = int(i[-2:])\n",
    "        date.append(datetime.datetime(year,month,day).strftime('%s'))\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdate = transDate(date.FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "def convertTime(t):\n",
    "    x = time.strptime(t,'%Y-%M-%D')\n",
    "    return str(int(datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-06',\n",
       " '2016-01-06',\n",
       " '2016-01-13',\n",
       " '2016-01-13',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-20',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-01-27',\n",
       " '2016-02-17',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-17',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-24',\n",
       " '2016-02-17',\n",
       " '2016-02-17']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(date.FD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1422835200'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime(2015,02,02).strftime('%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1422921600'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime(2015,2,3).strftime('%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "timestamps = []\n",
    "for timestamp in fileinput.input('alltimes'):\n",
    "    num_titles = int(timestamp.strip().split()[0])\n",
    "    timestamp = float(timestamp.strip().split()[1])\n",
    "    timestamps.extend([timestamp for title in range(num_titles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0,\n",
       " -1734462000.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1734462000.0 1111467600.0\n"
     ]
    }
   ],
   "source": [
    "fir_timestamp = timestamps[0]\n",
    "ast_timestamp = timestamps[len(timestamps)-1]\n",
    "print fir_timestamp,  ast_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1734462000.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111467600.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111467600.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tot import TopicsOverTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultspath = 'results/'\n",
    "documents_path = 'data_seg3.txt'\n",
    "timestamps_path = 'date.txt'\n",
    "stopwords_path = 'filterwords'\n",
    "tot = TopicsOverTime()\n",
    "documents, timestamps, dictionary = tot.GetPnasCorpusAndDictionary(documents_path, timestamps_path, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚丙烯 材料涉及 改性 阻燃 聚丙烯 材料阻燃 聚丙烯 含有 聚丙烯阻燃剂阻燃 协效剂阻燃剂三聚氰胺磷酸盐 聚磷酸铵 季戊四醇 至少 阻燃 协效剂 选自 改性 本法 发明 阻燃 聚丙烯 材料 阻燃 阻燃性 提升 阻燃 材料 力学性能\n"
     ]
    }
   ],
   "source": [
    "print ' '.join(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input() already active",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d9ae59ea18c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstopwords_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'filterwords'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zack/anaconda2/lib/python2.7/fileinput.pyc\u001b[0m in \u001b[0;36minput\u001b[1;34m(files, inplace, backup, bufsize, mode, openhook)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_state\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input() already active\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0m_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopenhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input() already active"
     ]
    }
   ],
   "source": [
    "stopwords_path = 'filterwords'\n",
    "for line in fileinput.input(stopwords_path):\n",
    "    stopwords.update(set(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords.update(set(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords.add('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('filterwords', 'r') as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip().decode('utf-8'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input() already active",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-4243564d3108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m#words = [word for word in doc.lower().strip().split() if word not in stopwords]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#dictionary.update(set(words))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/zack/anaconda2/lib/python2.7/fileinput.pyc\u001b[0m in \u001b[0;36minput\u001b[1;34m(files, inplace, backup, bufsize, mode, openhook)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_state\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input() already active\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0m_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFileInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopenhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input() already active"
     ]
    }
   ],
   "source": [
    "for doc in fileinput.input(documents_path):\n",
    "    #words = [word for word in doc.lower().strip().split() if word not in stopwords]\n",
    "    words = [word for word in doc.strip().split()]\n",
    "    documents.append(words)\n",
    "    #dictionary.update(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents_path = 'data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "with open(documents_path, 'r') as f:\n",
    "    for line in f:\n",
    "        if stopwords is not None:\n",
    "            words = [word for word in line.strip().decode('utf-8').split() if word not in stopwords]\n",
    "        else: \n",
    "            words = [word for word in line.strip().decode('utf-8').split()]\n",
    "        documents.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "\n",
    "with open(timestamps_path, 'r') as f:\n",
    "    for line in f:\n",
    "        timestamps.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1 = ' '.join(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('try', 'w') as f:\n",
    "    f.write(str1.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('try', 'r') as f:\n",
    "    str2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tot2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tot2.py\n",
    "# Copyright 2015 Abhinav Maurya\n",
    "\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "\n",
    "import fileinput\n",
    "import random\n",
    "import scipy.special\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pickle\n",
    "from math import log\n",
    "\n",
    "class TopicsOverTime:\n",
    "    def GetPnasCorpusAndDictionary(self, documents_path, timestamps_path, stopwords_path=None):\n",
    "        documents = []\n",
    "        timestamps = []\n",
    "        dictionary = set()\n",
    "\n",
    "        # read stopwords\n",
    "\n",
    "        if stopwords_path is not None:\n",
    "            stopwords = set()\n",
    "            with open('filterwords', 'r') as f:\n",
    "                for line in f:\n",
    "                    stopwords.append(line.strip().decode('utf-8'))\n",
    "\n",
    "        # read documents\n",
    "\n",
    "        with open(documents_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if stopwords is not None:\n",
    "                    words = [word for word in line.strip().decode('utf-8').split() if word not in stopwords]\n",
    "                else: \n",
    "                    words = [word for word in line.strip().decode('utf-8').split()]\n",
    "                documents.append(words)\n",
    "                dictionary.update(set(words))\n",
    "\n",
    "        # read time data\n",
    "\n",
    "        with open(timestamps_path, 'r') as f:\n",
    "            for line in f:\n",
    "                timestamps.append(int(line.strip()))\n",
    "\n",
    "\n",
    "        min_timestamp = min(timestamps)\n",
    "        max_timestamp = max(timestamps)\n",
    "        timestamps = [1.0*(t-min_timestamp)/(max_timestamp-min_timestamp) for t in timestamps]\n",
    "        dictionary = list(dictionary)\n",
    "        assert len(documents) == len(timestamps)\n",
    "        return documents, timestamps, dictionary\n",
    "\n",
    "    def CalculateCounts(self, par):\n",
    "        for d in range(par['D']):\n",
    "            for i in range(par['N'][d]):\n",
    "                topic_di = par['z'][d][i]\t\t#topic in doc d at position i\n",
    "                word_di = par['w'][d][i]\t\t#word ID in doc d at position i\n",
    "                par['m'][d][topic_di] += 1\n",
    "                par['n'][topic_di][word_di] += 1\n",
    "                par['n_sum'][topic_di] += 1\n",
    "\n",
    "    def InitializeParameters(self, documents, timestamps, dictionary, n_topics = 20, max_iter = 250):\n",
    "        par = {}\t\t\t\t\t\t# dictionary of all parameters\n",
    "        par['dataset'] = 'pnas'\t\t\t# dataset name\n",
    "        par['max_iterations'] = max_iter\t\t# max number of iterations in gibbs sampling\n",
    "        par['T'] = n_topics\t\t\t\t\t# number of topics\n",
    "        par['D'] = len(documents)\n",
    "        par['V'] = len(dictionary)\n",
    "        par['N'] = [len(doc) for doc in documents]\n",
    "        par['alpha'] = [50.0/par['T'] for _ in range(par['T'])]\n",
    "        par['beta'] = [0.1 for _ in range(par['V'])]\n",
    "        par['beta_sum'] = sum(par['beta'])\n",
    "        par['psi'] = [[1 for _ in range(2)] for _ in range(par['T'])]\n",
    "        par['betafunc_psi'] = [scipy.special.beta( par['psi'][t][0], par['psi'][t][1] ) for t in range(par['T'])]\n",
    "        par['word_id'] = {dictionary[i]: i for i in range(len(dictionary))}\n",
    "        par['word_token'] = dictionary\n",
    "        par['z'] = [[random.randrange(0,par['T']) for _ in range(par['N'][d])] for d in range(par['D'])]\n",
    "        par['t'] = [[timestamps[d] for _ in range(par['N'][d])] for d in range(par['D'])]\n",
    "        par['w'] = [[par['word_id'][documents[d][i]] for i in range(par['N'][d])] for d in range(par['D'])]\n",
    "        par['m'] = [[0 for t in range(par['T'])] for d in range(par['D'])]\n",
    "        par['n'] = [[0 for v in range(par['V'])] for t in range(par['T'])]\n",
    "        par['n_sum'] = [0 for t in range(par['T'])]\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        np.seterr(divide='ignore', invalid='ignore')\n",
    "        self.CalculateCounts(par)\n",
    "        return par\n",
    "\n",
    "    def GetTopicTimestamps(self, par):\n",
    "        topic_timestamps = []\n",
    "        for topic in range(par['T']):\n",
    "            current_topic_timestamps = []\n",
    "            current_topic_doc_timestamps = [[ (par['z'][d][i]==topic)*par['t'][d][i] for i in range(par['N'][d])] for d in range(par['D'])]\n",
    "            for d in range(par['D']):\n",
    "                current_topic_doc_timestamps[d] = filter(lambda x: x!=0, current_topic_doc_timestamps[d])\n",
    "            for timestamps in current_topic_doc_timestamps:\n",
    "                current_topic_timestamps.extend(timestamps)\n",
    "            assert current_topic_timestamps != []\n",
    "            topic_timestamps.append(current_topic_timestamps)\n",
    "        return topic_timestamps\n",
    "\n",
    "    def GetMethodOfMomentsEstimatesForPsi(self, par):\n",
    "        topic_timestamps = self.GetTopicTimestamps(par)\n",
    "        psi = [[1 for _ in range(2)] for _ in range(len(topic_timestamps))]\n",
    "        for i in range(len(topic_timestamps)):\n",
    "            current_topic_timestamps = topic_timestamps[i]\n",
    "            timestamp_mean = np.mean(current_topic_timestamps)\n",
    "            timestamp_var = np.var(current_topic_timestamps)\n",
    "            if timestamp_var == 0:\n",
    "                timestamp_var = 1e-6\n",
    "            common_factor = timestamp_mean*(1-timestamp_mean)/timestamp_var - 1\n",
    "            psi[i][0] = 1 + timestamp_mean*common_factor\n",
    "            psi[i][1] = 1 + (1-timestamp_mean)*common_factor\n",
    "        return psi\n",
    "\n",
    "    def ComputePosteriorEstimatesOfThetaAndPhi(self, par):\n",
    "        theta = deepcopy(par['m'])\n",
    "        phi = deepcopy(par['n'])\n",
    "\n",
    "        for d in range(par['D']):\n",
    "            if sum(theta[d]) == 0:\n",
    "                theta[d] = np.asarray([1.0/len(theta[d]) for _ in range(len(theta[d]))])\n",
    "            else:\n",
    "                theta[d] = np.asarray(theta[d])\n",
    "                theta[d] = 1.0*theta[d]/sum(theta[d])\n",
    "        theta = np.asarray(theta)\n",
    "\n",
    "        for t in range(par['T']):\n",
    "            if sum(phi[t]) == 0:\n",
    "                phi[t] = np.asarray([1.0/len(phi[t]) for _ in range(len(phi[t]))])\n",
    "            else:\n",
    "                phi[t] = np.asarray(phi[t])\n",
    "                phi[t] = 1.0*phi[t]/sum(phi[t])\n",
    "        phi = np.asarray(phi)\n",
    "\n",
    "        return theta, phi\n",
    "\n",
    "    def ComputePosteriorEstimatesOfTheta(self, par):\n",
    "        theta = deepcopy(par['m'])\n",
    "\n",
    "        for d in range(par['D']):\n",
    "            if sum(theta[d]) == 0:\n",
    "                theta[d] = np.asarray([1.0/len(theta[d]) for _ in range(len(theta[d]))])\n",
    "            else:\n",
    "                theta[d] = np.asarray(theta[d])\n",
    "                theta[d] = 1.0*theta[d]/sum(theta[d])\n",
    "\n",
    "        return np.matrix(theta)\n",
    "\n",
    "    def ComputePosteriorEstimateOfPhi(self, par):\n",
    "        phi = deepcopy(par['n'])\n",
    "\n",
    "        for t in range(par['T']):\n",
    "            if sum(phi[t]) == 0:\n",
    "                phi[t] = np.asarray([1.0/len(phi[t]) for _ in range(len(phi[t]))])\n",
    "            else:\n",
    "                phi[t] = np.asarray(phi[t])\n",
    "                phi[t] = 1.0*phi[t]/sum(phi[t])\n",
    "\n",
    "        return np.matrix(phi)\n",
    "\n",
    "    def TopicsOverTimeGibbsSampling(self, par):\n",
    "        for iteration in range(par['max_iterations']):\n",
    "            for d in range(par['D']):\n",
    "                for i in range(par['N'][d]):\n",
    "                    word_di = par['w'][d][i]\n",
    "                    t_di = par['t'][d][i]\n",
    "\n",
    "                    old_topic = par['z'][d][i]\n",
    "                    par['m'][d][old_topic] -= 1\n",
    "                    par['n'][old_topic][word_di] -= 1\n",
    "                    par['n_sum'][old_topic] -= 1\n",
    "\n",
    "                    topic_probabilities = []\n",
    "                    for topic_di in range(par['T']):\n",
    "                        psi_di = par['psi'][topic_di]\n",
    "                        topic_probability = 1.0 * (par['m'][d][topic_di] + par['alpha'][topic_di])\n",
    "                        topic_probability *= ((1-t_di)**(psi_di[0]-1)) * ((t_di)**(psi_di[1]-1))\n",
    "                        topic_probability /= par['betafunc_psi'][topic_di]\n",
    "                        topic_probability *= (par['n'][topic_di][word_di] + par['beta'][word_di])\n",
    "                        topic_probability /= (par['n_sum'][topic_di] + par['beta_sum'])\n",
    "                        topic_probabilities.append(topic_probability)\n",
    "                    sum_topic_probabilities = sum(topic_probabilities)\n",
    "                    if sum_topic_probabilities == 0:\n",
    "                        topic_probabilities = [1.0/par['T'] for _ in range(par['T'])]\n",
    "                    else:\n",
    "                        topic_probabilities = [p/sum_topic_probabilities for p in topic_probabilities]\n",
    "\n",
    "                    new_topic = list(np.random.multinomial(1, topic_probabilities, size=1)[0]).index(1)\n",
    "                    par['z'][d][i] = new_topic\n",
    "                    par['m'][d][new_topic] += 1\n",
    "                    par['n'][new_topic][word_di] += 1\n",
    "                    par['n_sum'][new_topic] += 1\n",
    "\n",
    "                if d%1000 == 0:\n",
    "                    print('Done with iteration {iteration} and document {document}'.format(iteration=iteration, document=d))\n",
    "            par['psi'] = self.GetMethodOfMomentsEstimatesForPsi(par)\n",
    "            par['betafunc_psi'] = [scipy.special.beta( par['psi'][t][0], par['psi'][t][1] ) for t in range(par['T'])]\n",
    "        par['m'], par['n'] = self.ComputePosteriorEstimatesOfThetaAndPhi(par)\n",
    "        return par['m'], par['n'], par['psi']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
